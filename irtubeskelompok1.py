# -*- coding: utf-8 -*-
"""IRtubeskelompok1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HrapuQVrj4wkbC_ehigI48VOd9K3S6lR
"""

!pip install ir_datasets sentence-transformers faiss-cpu ir_measures

import ir_datasets

dataset = ir_datasets.load("msmarco-passage/train")

print(dataset)  # ini akan menampilkan struktur lengkap

# Cek struktur fields dari setiap komponen
print("Fields in docs:", next(dataset.docs_iter())._fields)
print("Fields in queries:", next(dataset.queries_iter())._fields)
print("Fields in qrels:", next(dataset.qrels_iter())._fields)

# Lihat isi pertama dari masing-masing komponen
print("\n=== Sample docs ===")
print(next(dataset.docs_iter()))

print("\n=== Sample queries ===")
print(next(dataset.queries_iter()))

print("\n=== Sample qrels ===")
print(next(dataset.qrels_iter()))

# Hitung jumlah masing-masing komponen
num_docs = sum(1 for _ in dataset.docs_iter())
num_queries = sum(1 for _ in dataset.queries_iter())
num_qrels = sum(1 for _ in dataset.qrels_iter())
# Tampilkan hasil
print(f"Jumlah dokumen (docs): {num_docs:,}")
print(f"Jumlah query (queries): {num_queries:,}")
print(f"Jumlah qrels (label relevansi): {num_qrels:,}")

from itertools import islice

# Ambil sample dari masing-masing komponen (5 baris)
docs_sample = list(islice(dataset.docs_iter(), 5))
queries_sample = list(islice(dataset.queries_iter(), 5))
qrels_sample = list(islice(dataset.qrels_iter(), 5))

import pandas as pd
# Buat dataframe untuk masing-masing sample
docs_df = pd.DataFrame([{"doc_id": d.doc_id, "text": d.text[:300]} for d in docs_sample])
queries_df = pd.DataFrame([{"query_id": q.query_id, "query": q.text} for q in queries_sample])
qrels_df = pd.DataFrame([{
    "query_id": r.query_id,
    "doc_id": r.doc_id,
    "relevance": r.relevance,
    "iteration": r.iteration
} for r in qrels_sample])

docs_df

queries_df

qrels_df

from sentence_transformers import SentenceTransformer
import torch

# Gunakan model pretrained yang mirip ANCE (latihannya di MS MARCO juga)
model = SentenceTransformer("sentence-transformers/msmarco-distilbert-base-tas-b")

# Ambil teks dokumen (subset 10.000 dulu biar ringan)
docs_subset = list(dataset.docs_iter())[:10000]
doc_texts = [d.text for d in docs_subset]

# Encode dokumen jadi vector
doc_embeddings = model.encode(doc_texts, convert_to_tensor=True, show_progress_bar=True)

"""indexing"""

import faiss

# Buat FAISS Index (inner product sesuai jurnal)
index = faiss.IndexFlatIP(doc_embeddings.shape[1])

# Normalisasi dokumen dan masukkan ke index
faiss.normalize_L2(doc_embeddings.numpy())
index.add(doc_embeddings.numpy())

# ==== Sistem Interaktif: Input Query Manual ====
# Pastikan doc_embeddings, docs_subset, model, index sudah ada dari cell sebelumnya

# Input query dari user
user_query = input("Masukkan query anda: ")

# Encode query
query_emb = model.encode(user_query, convert_to_tensor=True)
query_emb_norm = query_emb / query_emb.norm()

# Retrieval awal (tanpa PRF)
top_k = 10
scores, indices = index.search(query_emb_norm.unsqueeze(0).numpy(), top_k) # fungsi FAISS bekerja disini

# Hitung PRF vector dari top-k hasil
beta = 0.8
top_doc_embs = doc_embeddings[indices[0]]
prf_vector = torch.mean(top_doc_embs, dim=0)
enhanced_query = query_emb + beta * prf_vector
enhanced_query_norm = enhanced_query / enhanced_query.norm()

# Retrieval ulang (dengan PRF)
scores_prf, indices_prf = index.search(enhanced_query_norm.unsqueeze(0).numpy(), top_k)

# Tampilkan hasil
print("\\n=== Hasil SEBELUM PRF ===")
for i in range(3):
    print(f"- Score: {scores[0][i]:.4f}")
    print(f"  Doc: {docs_subset[indices[0][i]].text[:200]}\\n")

print("=== Hasil SESUDAH PRF ===")
for i in range(3):
    print(f"- Score: {scores_prf[0][i]:.4f}")
    print(f"  Doc: {docs_subset[indices_prf[0][i]].text[:200]}\\n")

import pickle
import torch

# Simpan dokumen
with open("docs_subset.pkl", "wb") as f:
    pickle.dump(docs_subset, f)

# Simpan embeddings
torch.save(doc_embeddings, "doc_embeddings.pt")

"""buat file app.py untuk streamlit"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import torch
# import pickle
# from sentence_transformers import SentenceTransformer, util
# import faiss
# 
# st.set_page_config(page_title="ANCE-PRF Demo", layout="centered")
# st.title("üìö ANCE-PRF Interactive IR System")
# 
# # Load model dan data
# @st.cache_resource
# def load_model_and_data():
#     model = SentenceTransformer("sentence-transformers/msmarco-distilbert-base-tas-b")
#     with open("docs_subset.pkl", "rb") as f:
#         docs_subset = pickle.load(f)
#     doc_embeddings = torch.load("doc_embeddings.pt")
#     faiss.normalize_L2(doc_embeddings.numpy())
#     index = faiss.IndexFlatIP(doc_embeddings.shape[1])
#     index.add(doc_embeddings.numpy())
#     return model, docs_subset, doc_embeddings, index
# 
# model, docs_subset, doc_embeddings, index = load_model_and_data()
# 
# # Input query
# query = st.text_input("Masukkan pertanyaan Anda:")
# 
# if query:
#     with st.spinner("Sedang mencari dokumen relevan..."):
#         # Encode query
#         query_emb = model.encode(query, convert_to_tensor=True)
#         query_emb_norm = query_emb / query_emb.norm()
# 
#         # Retrieval awal
#         top_k = 10
#         scores, indices = index.search(query_emb_norm.unsqueeze(0).numpy(), top_k)
# 
#         # PRF vector
#         beta = 0.8
#         top_doc_embs = doc_embeddings[indices[0]]
#         prf_vector = torch.mean(top_doc_embs, dim=0)
#         enhanced_query = query_emb + beta * prf_vector
#         enhanced_query_norm = enhanced_query / enhanced_query.norm()
# 
#         # Retrieval ulang
#         scores_prf, indices_prf = index.search(enhanced_query_norm.unsqueeze(0).numpy(), top_k)
# 
#         # Tampilkan hasil
#         st.markdown("### üîç Hasil *SEBELUM* PRF")
#         for i in range(3):
#             st.write(f"**Score:** {scores[0][i]:.4f}")
#             st.write(docs_subset[indices[0][i]].text[:300])
#             st.markdown("---")
# 
#         st.markdown("### üöÄ Hasil *SESUDAH* PRF")
#         for i in range(3):
#             st.write(f"**Score:** {scores_prf[0][i]:.4f}")
#             st.write(docs_subset[indices_prf[0][i]].text[:300])
#             st.markdown("---")

!pip install streamlit pyngrok --quiet

"""Authtoken ke ngrok untuk streamlit"""

!ngrok config add-authtoken 2zmIYam4Qz3pj4sNpgzsZ83gbmE_3RFMaxXTWccnT9w9TEqCr

import subprocess, time
from pyngrok import ngrok

# Jalankan streamlit di background
process = subprocess.Popen(["streamlit", "run", "app.py"])
time.sleep(10)

# Buka tunnel
public_url = ngrok.connect(8501)
print("üîó Streamlit kamu bisa diakses di:", public_url)

import subprocess, time
from pyngrok import ngrok

# üîÅ Tutup semua tunnel aktif sebelumnya (penting!)
ngrok.kill()

# üü¢ Jalankan Streamlit app
process = subprocess.Popen(["streamlit", "run", "app.py"])

# ‚è≥ Tunggu agar Streamlit siap (bisa disesuaikan waktunya)
time.sleep(10)

# üåê Buka ngrok tunnel
public_url = ngrok.connect(8501)
print("üîó Streamlit kamu bisa diakses di:", public_url)

import ir_datasets

dataset = ir_datasets.load("msmarco-passage/dev/small")
queries = list(dataset.queries_iter())

# Tampilkan 10 query pertama
for q in queries[:10]:
    print(f"Query ID: {q.query_id} ‚Äî Text: {q.text}")

"""metrik evaluasi"""

!pip install ir_measures

from ir_measures import calc_aggregate, MRR, nDCG, Recall

subset_doc_ids = set([doc.doc_id for doc in docs_subset])
print(f"Jumlah dokumen di subset: {len(subset_doc_ids)}")

import ir_datasets
dataset = ir_datasets.load("msmarco-passage/dev/small")
qrels = list(dataset.qrels_iter())

# Filter qrels yang doc_id-nya ada di subset
relevant_qrels = [q for q in qrels if q.doc_id in subset_doc_ids]

# Ambil query_id yang punya relasi dengan subset
query_ids_in_subset = set([q.query_id for q in relevant_qrels])

print(f"Jumlah query relevan dengan subset dokumen: {len(query_ids_in_subset)}")

queries = list(dataset.queries_iter())
query_dict = {q.query_id: q.text for q in queries}

# Ambil 5 query yang relevan
for qid in list(query_ids_in_subset)[:5]:
    print(f"Query ID: {qid} ‚Äî Text: {query_dict[qid]}")